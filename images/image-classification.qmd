---
title: "Image classification"
subtitle: Putting images into buckets
order: 1
custom-header: ../assets/wat-3.png
---

<div style="display: none;">

![](../assets/wat-3.png)

</div>


Image classification allows you to put images into category. Typically these are yes/no questions.

* Does this satellite image show illegal mining operations?
* Does this photograph have a car in it?
* Does this x-ray show a broken bone?

Custom image classifiers are simple to build yourself! You can visit the [build your own image classifier page](../diy/image-classification.qmd) for more details.

Image classifiers are on the simpler end of the visual analysis spectrum. More advanced models can show exactly where things are in an image (e.g. vegetation, swimming pools), or segment out those individual things (e.g. cars, people). Keep reading to learn more about semantic segmentation and object detection.

:::{.callout-info}
You might also be interested in [models that can describe images](image-description.qmd), which can often also do classification!
:::

## Try it out

In the [intro to fine-tuning](../intro/fine-tune-model.qmd) we build a [detector for illegal amber mines](https://huggingface.co/spaces/wendys-llc/wendys-llc-amber-mines). Click the examples below to see how it performs (although let's be honest: right now I'm cheating by giving it images it's already seen).

<script
	type="module"
	src="https://gradio.s3-us-west-2.amazonaws.com/3.22.1/gradio.js"
></script>

<gradio-app src="https://wendys-llc-wendys-llc-amber-mines.hf.space"></gradio-app>

## Python code

In this example we're going to use **zero-shot image classification** to see whether a painting is of a woman or a man. It's called "zero-shot" because we aren't customizing a model specifically for the task, but relying on the [base model](https://huggingface.co/openai/clip-vit-large-patch14) to know the difference between the two.

```python
from PIL import Image
import requests
import numpy as np

from transformers import CLIPProcessor, CLIPModel
```

...then use (more or less) the example right from [the documentation](https://huggingface.co/openai/clip-vit-large-patch14):

```python
url = "https://i.etsystatic.com/5554021/r/il/31ad32/644701138/il_1588xN.644701138_6gh4.jpg"
image = Image.open(requests.get(url, stream=True).raw)

choices = ["a painting of a woman", "a painting of a man"]
inputs = processor(text=choices, images=image, return_tensors="pt", padding=True)
outputs = model(**inputs)

logits_per_image = outputs.logits_per_image # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities

# Probabilities for each potential class
# We use [0] because we only passed one image
prediction_probs = probs.tolist()[0]
print(prediction_probs)
```

We're given two probabilities, in order matching the classes we offered. Below we use `np.argmax` to figure out what the higher index is (`0` or `1`) and then display that choice.

```python
# Which class?
prediction = np.argmax(prediction_probs)
print(choices[prediction])
```

This is *not* a fantastic prompt: what about groups? What about paintings of vegetables? But it's a start â€“ you aren't restricted to two choices, you're welcome to add as many as you'd like.